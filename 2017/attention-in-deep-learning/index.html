<!DOCTYPE html>
<html lang="en">
  <head>
  <meta http-equiv="content-type" content="text/html;charset=utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="robots" content="noodp"/>
  <meta name="google-site-verification" content="dzraMol4fk4b15wv9pMw_wWCtD8m61QU4tGK-3BFGLQ" />
  
  
  
  <meta name=”baidu-site-verification” content="gvezHX9CB4" />
  <link rel="prev" href="https://billbeatthepeat.github.io/2017/little-skills/" />
  <link rel="next" href="https://billbeatthepeat.github.io/2017/ensemble-methods/" />
  <link rel="canonical" href="https://billbeatthepeat.github.io/2017/attention-in-deep-learning/" />
  <link rel='shortcut icon' type='image/x-icon' href='/favicon.ico' />
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">
  <title>
       
       
           Attention in Deep Learning | Junbin Huang
       
  </title>
  <meta name="title" content="Attention in Deep Learning | Junbin Huang">
    
  
  <link rel="stylesheet" href="/font/iconfont.css">
  <link rel="stylesheet" href="/css/main.min.css">


  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Attention in Deep Learning"/>
<meta name="twitter:description" content="This article is a digest/note for the blog of Denny Britz: Attention ad Memory in Deep Learning and NLP.
 In Neural Machine Translation system, we map the meaning of a sentence into a fixed-length vector representation and then generate a translation base on it.
The first word of English translation is probably highly correlated ith the first word of the source sentence. Researches have found that reversing the source sequence (feeding it backwards into the encoder) produce significantly better results because it shorten the path from the decoder to the relevan parts of encoder."/>

  <script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "headline": "Attention in Deep Learning",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https:\/\/billbeatthepeat.github.io\/2017\/attention-in-deep-learning\/"
  },
  "image": {
    "@type": "ImageObject",
    "url": "https:\/\/billbeatthepeat.github.io\/cover.png",
    "width":  800 ,
    "height":  600 
  },
  "genre": "posts",
  
  "wordcount":  329 ,
  "url": "https:\/\/billbeatthepeat.github.io\/2017\/attention-in-deep-learning\/",
  "datePublished": "2017-09-25T18:16:28\x2b00:00",
  "dateModified": "2017-09-25T18:16:28\x2b00:00",
  
  "publisher": {
    "@type": "Organization",
    "name": "Junbin Huang",
    "logo": {
      "@type": "ImageObject",
      "url": "https:\/\/billbeatthepeat.github.io\/logo.png",
      "width":  127 ,
      "height":  40 
    }
  },
  "author": {
    "@type": "Person",
    "name": "Junbin Huang"
  },
  "description": ""
}
</script>
</head>

  



  <body class="">
    <div class="wrapper">
        <nav class="navbar">
    <div class="container">
        <div class="navbar-header header-logo">
        	<a href="https://billbeatthepeat.github.io/">Junbin Huang</a>
        </div>
        <div class="menu navbar-right">
                
                
                <a class="menu-item" href="/posts" title="">Blog</a>
                
                <a class="menu-item" href="/categories" title="">Categories</a>
                
                <a class="menu-item" href="/categories/top/" title="">Tops</a>
                
                <a class="menu-item" href="/about" title="">About</a>
                
                <a class="menu-item" href="/cv" title="">CV</a>
                
                <a href="javascript:void(0);" class="theme-switch"><i class="iconfont icon-sun"></i></a>&nbsp;
        </div>
    </div>
</nav>
<nav class="navbar-mobile" id="nav-mobile" style="display: none">
     <div class="container">
        <div class="navbar-header">
            <div class="mdl-card__media mdl-color-text--grey-50" style="background-image:url(/images/IMG_1825.png)">
        <p class="index-top-block-slogan"><a href="#">
        
            
                Artificial Intelligence is the new electricity. -- Andrew Ng
            
        
        </a></p>
        </div>
            <div>  <a href="javascript:void(0);" class="theme-switch"><i class="iconfont icon-sun"></i></a>&nbsp;<a href="https://billbeatthepeat.github.io/">Junbin Huang</a></div>
            <div class="menu-toggle">
                <span></span><span></span><span></span>
            </div>
        </div>
     
          <div class="menu" id="mobile-menu">
                
                
                <a class="menu-item" href="/posts" title="">Blog</a>
                
                <a class="menu-item" href="/categories" title="">Categories</a>
                
                <a class="menu-item" href="/categories/top/" title="">Tops</a>
                
                <a class="menu-item" href="/about" title="">About</a>
                
                <a class="menu-item" href="/cv" title="">CV</a>
                
        </div>
    </div>
</nav>

    	 <main class="main">
          <div class="container">
      		
<article class="post-warp">
    <header class="post-header">
        <h1 class="post-title">Attention in Deep Learning</h1>
        <div class="post-meta">
                <span class="post-time">
                    on <time datetime=2017-09-25 >25 September 2017</time>
                </span>
                in
                <i class="iconfont icon-folder"></i>
                <span class="post-category">
                        <a href="https://billbeatthepeat.github.io/categories/machine-learning/"> Machine Learning </a>
                        
                </span>
                <i class="iconfont icon-timer"></i>
                2 min
        </div>
    </header>
    <div class="post-content">
        

        

        
        
     
          
          
          

          
          
          

          <blockquote>
<p>This article is a digest/note for the blog of Denny Britz:  <a href="http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/">Attention ad Memory in Deep Learning and NLP</a>.</p>
</blockquote>
<p>In Neural Machine Translation system, we map the meaning of a sentence into a fixed-length vector representation and then generate a translation base on it.</p>
<p>The first word of English translation is probably highly correlated ith the first word of the source sentence. Researches have found that reversing the source sequence (feeding it backwards into the encoder) produce significantly better results because it shorten the path from the decoder to the relevan parts of encoder. Similarly, feeding an input twice also seems to help a network to better memorize things. But this hack is not always useful.</p>
<p>With an attention mechanism, we no longer try encode the full source sentence into a fix-length vector. Rather, we allow the decoder to &ldquo;attend&rdquo; to different parts of the source sentence at each step of the output generation. Importantly, we let the model learn what to attend to based on the input sentence and what it has produced so far. The important part is that each decoder output word <strong>yt</strong> now depends on a weighted combination of all the input states, not just the last state.</p>
<p>The weight value a&rsquo;s are the &ldquo;attention&rdquo; the decoder pays to the question. The a&rsquo;s are typically normalized to sum to 1 (so they are a distribution over the input state).</p>
<p>A big advantage of attention i taht it gives us the ability to interpret an visualize what the model is doing.</p>
<p>An alternative approach to attention is to use Reinforcement Learning to predict an approximate location to focus to.</p>
<p>The basic problem that the attention mechanism solves is that it allow the network to refer back to the input sequence, instead of forcing it to encode all information  into a fixed-length vector. Interpreted another way, the attention mechanism is simply giving the network access to its internal memory, which is the hidden state of th encoder.</p>

    </div>

    <div class="post-copyright">
             
            <p class="copyright-item">
                <span>Author:</span>
                <span>Junbin Huang </span>
                </p>
            
           
            <p class="copyright-item">
                    <span>Words:</span>
                   <span>329</span>
            </p>
            
            <p class="copyright-item">
                
                <span>Share:</span>
                <span>

      
        <a href="//twitter.com/share?url=https%3a%2f%2fbillbeatthepeat.github.io%2f2017%2fattention-in-deep-learning%2f&amp;text=Attention%20in%20Deep%20Learning&amp;via=" target="_blank" title="Share on Twitter">
          <i class="iconfont icon-twitter"></i>
        </a>
        
      
      
        <a href="//www.facebook.com/sharer/sharer.php?u=https%3a%2f%2fbillbeatthepeat.github.io%2f2017%2fattention-in-deep-learning%2f" target="_blank" title="Share on Facebook">
          <i class="iconfont icon-facebook"></i>
        </a>
        
      
      
      
      
        <a href="//www.linkedin.com/shareArticle?url=https%3a%2f%2fbillbeatthepeat.github.io%2f2017%2fattention-in-deep-learning%2f&amp;title=Attention%20in%20Deep%20Learning" target="_blank" title="Share on LinkedIn">
          <i class="iconfont icon-linkedin"></i>
        </a>
        
      
      
        
      
        
      

          

          

          

          
</span>
                
            </p>

             
            <p class="copyright-item">
                Released under <a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a>
            </p>
            
    </div>

  
    <div class="post-tags">
        
        <section>
                <a href="javascript:window.history.back();">Back</a></span> · 
                <span><a href="https://billbeatthepeat.github.io/">Home</a></span>
        </section>
    </div>

    <div class="post-nav">
        
        <a href="https://billbeatthepeat.github.io/2017/little-skills/" class="prev" rel="prev" title="Little Skills"><i class="iconfont icon-dajiantou"></i>&nbsp;Little Skills</a>
         
        
        <a href="https://billbeatthepeat.github.io/2017/ensemble-methods/" class="next" rel="next" title="Ensemble Methods">Ensemble Methods&nbsp;<i class="iconfont icon-xiaojiantou"></i></a>
        
    </div>

    <div class="post-comment">
          
          <div id="disqus_thread"></div>
  <script type="text/javascript">
      (function() {
          
          
          if (window.location.hostname == "localhost")
              return;
          var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
          var disqus_shortname = 'jhuan129';
          dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="https://disqus.com/" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

 

          
    </div>
</article>
          </div>
		   </main>
      <footer class="footer">
    <div class="copyright">
        &copy;
        
        <span itemprop="copyrightYear">2020 - 2020</span>
        
         
            <span class="author" itemprop="copyrightHolder"><a href="https://billbeatthepeat.github.io/">Junbin Huang</a> | </span>
         

		  <span>Supported by <a href="https://github.com/Fastbyte01/KeepIt" target="_blank" rel="external nofollow noopener noreffer">KeepIt</a> & <a href="https://gohugo.io/" target="_blank" rel="external nofollow noopener noreffer">Hugo</a></span>
    </div>
</footer>
<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>












    
    
    <script src="/js/vendor_no_gallery.min.js" async=""></script>
    
  







     </div>
  </body>
</html>
