<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Junbin Huang</title>
    <link>https://billbeatthepeat.github.io/posts/</link>
    <description>Recent content in Posts on Junbin Huang</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 03 May 2020 10:59:01 -0700</lastBuildDate>
    
	<atom:link href="https://billbeatthepeat.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Currently_focus</title>
      <link>https://billbeatthepeat.github.io/2020/currently-focus/</link>
      <pubDate>Sun, 19 Apr 2020 14:32:50 -0700</pubDate>
      
      <guid>https://billbeatthepeat.github.io/2020/currently-focus/</guid>
      <description>Start date: 2020-01-01
Recent Tasks  In Progress:  ã€ŠMathematics for Machine Learningã€‹ ã€ŠFluent Pythonã€‹ Documentary about western art history Surfing ğŸ„ğŸ»â€â™‚ï¸ Snowboarding ğŸ‚ğŸ» Bodybuilding ğŸ‹ğŸ»â€â™‚ï¸ and Hiding from crowds ğŸ¥·ğŸ»   Plan to doï¼š  ã€ŠMathematics for Machine Learningã€‹ Probability Course in CS109 or CS109. Linear Algebra in MIT 18.06. ã€ŠFluent Pythonã€‹ ã€Šæ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–å™¨çš„è‰ºæœ¯ï¼šåŸç†è§£æä¸SQLæ€§èƒ½ä¼˜åŒ–ã€‹. OCR Papers with code, implemented in Pytorch/MxNet $\color{blue}{[Long-term\ running\ plan]}$ . Review of Abnormal Detection as a blog/category $\color{blue}{[Long-term\ running\ plan]}$ .</description>
    </item>
    
    <item>
      <title>Projects</title>
      <link>https://billbeatthepeat.github.io/2020/projects/</link>
      <pubDate>Sun, 19 Apr 2020 14:31:24 +0000</pubDate>
      
      <guid>https://billbeatthepeat.github.io/2020/projects/</guid>
      <description>Lecture Notes  Stanford University, CS 131 Computer Vision: Foundations and Applications.  Note1,    Personal Project   Personal blog website BILLBEATTHEPEAT.github.io.
 Based on Hexo framework and the Material theme. Contains my resume and CV. Contains my blog of notes, projects I am working on, papers I have read and some life details.    Interesting things. Contains something I thought interesting and can be done by coding.</description>
    </item>
    
    <item>
      <title>Structuring Machine Learning Project</title>
      <link>https://billbeatthepeat.github.io/2020/structuring-machine-learning/</link>
      <pubDate>Sun, 03 May 2020 10:59:01 -0700</pubDate>
      
      <guid>https://billbeatthepeat.github.io/2020/structuring-machine-learning/</guid>
      <description>Class: ENG Created: Apr 19, 2020 10:29 PM Materials: https://www.coursera.org/learn/machine-learning-projects/home/welcome Reviewed: No Source: Coursera
Note from the Coursera course: Structuring Machine Learning Project
Orthogonalization Definition: orthogonalization is the thought in tuning, in which situation each tuning knob can only does one thing.
In ML project, we expect our algorithm can work well in 4 main goals:
 low cost in training set; low cost in dev set; low cost in test set; performs well in real world  These 4 goals are hierarchical in a one by one order.</description>
    </item>
    
    <item>
      <title>Mordern Art Ideas</title>
      <link>https://billbeatthepeat.github.io/2020/mordern-art-ideas/</link>
      <pubDate>Sun, 19 Apr 2020 19:27:30 -0700</pubDate>
      
      <guid>https://billbeatthepeat.github.io/2020/mordern-art-ideas/</guid>
      <description>Modern Art &amp;amp; Ideas Class: ART Created: Feb 07, 2020 11:37 AM Materials: https://www.moma.org/learn/moma_learning/themes/ Reviewed: No Source: Coursera
Week 1: Introduction to Modern Art &amp;amp; Ideas Birth of Modern Art About late-19th-century artists broke with tradition to create art for the modern age. The birth of modernism and modern art can be traced to the Industrial Revolution from mid-18 century and lasted through the 19th century. The evolution of transportation changes the way that people exploring the world and their worldview.</description>
    </item>
    
    <item>
      <title>Recently Read Papers</title>
      <link>https://billbeatthepeat.github.io/2020/recently-read-paper/</link>
      <pubDate>Sun, 19 Apr 2020 14:43:19 +0000</pubDate>
      
      <guid>https://billbeatthepeat.github.io/2020/recently-read-paper/</guid>
      <description>I create this page to write down some notes and reflection about the research paper/blog I have read. Here are some of the most useful ones gathering from the recent project I am working on. They are separated by topics and projects. Some of the titles are followed by my brief comment which describe the mainly reason I choose them to put down here.
The Emotion Recognition   Zheng W L, Lu B L.</description>
    </item>
    
    <item>
      <title>What is the &#39;Risk&#39; in ML</title>
      <link>https://billbeatthepeat.github.io/2019/what-is-the-risk-in-ml/</link>
      <pubDate>Sat, 09 Mar 2019 22:34:10 +0000</pubDate>
      
      <guid>https://billbeatthepeat.github.io/2019/what-is-the-risk-in-ml/</guid>
      <description>Here is a review notes of Machine Learning.
There are several ways to measure the performance of ML. One is using loss function to measure the closeness between prediction output with the ground truth. The second way doesnâ€™t just want the prediction of one test data but all the possible input from (X,Y)âˆ¼PXY(X,Y) \sim P_{XY}. So given a cell draw randomly from the distribution the perform of this model can be represent as the Risk which is the average of the performance:</description>
    </item>
    
    <item>
      <title>Correct methods to &#39;copy&#39; a list in Python</title>
      <link>https://billbeatthepeat.github.io/2018/correct-methods-to-copy-a-list-in-python/</link>
      <pubDate>Sat, 27 Oct 2018 11:47:22 +0000</pubDate>
      
      <guid>https://billbeatthepeat.github.io/2018/correct-methods-to-copy-a-list-in-python/</guid>
      <description>&amp;gt;&amp;gt;&amp;gt; import copy &amp;gt;&amp;gt;&amp;gt; a = [[10], 20] &amp;gt;&amp;gt;&amp;gt; b = a[:] &amp;gt;&amp;gt;&amp;gt; c = list(a) &amp;gt;&amp;gt;&amp;gt; d = a * 1 &amp;gt;&amp;gt;&amp;gt; e = copy.copy(a) &amp;gt;&amp;gt;&amp;gt; f = copy.deepcopy(a) &amp;gt;&amp;gt;&amp;gt; a.append(21) &amp;gt;&amp;gt;&amp;gt; a[0].append(11) &amp;gt;&amp;gt;&amp;gt; print id(a), a 30553152 [[10, 11], 20, 21] &amp;gt;&amp;gt;&amp;gt; print id(b), b 44969816 [[10, 11], 20] &amp;gt;&amp;gt;&amp;gt; print id(c), c 44855664 [[10, 11], 20] &amp;gt;&amp;gt;&amp;gt; print id(d), d 44971832 [[10, 11], 20] &amp;gt;&amp;gt;&amp;gt; print id(e), e 44833088 [[10, 11], 20] &amp;gt;&amp;gt;&amp;gt; print id(f), f 44834648 [[10], 20] Note that when using a[:], a*1, copy.</description>
    </item>
    
    <item>
      <title>Positional Notations in Python</title>
      <link>https://billbeatthepeat.github.io/2018/positional-notation-in-python/</link>
      <pubDate>Sat, 27 Oct 2018 11:44:10 +0000</pubDate>
      
      <guid>https://billbeatthepeat.github.io/2018/positional-notation-in-python/</guid>
      <description>1. Hexadecimal to decimal: &amp;gt;&amp;gt;&amp;gt; int(&amp;#39;0xf&amp;#39;,16) 15 2. Binary to decimal: &amp;gt;&amp;gt;&amp;gt; int(&amp;#39;10100111110&amp;#39;,2) 1342 3. Octonary to decimal: &amp;gt;&amp;gt;&amp;gt; int(&amp;#39;17&amp;#39;,8) 15 4. Decimal to hexadecimal: &amp;gt;&amp;gt;&amp;gt; hex(1033) &amp;#39;0x409&amp;#39; 5. Binary to hexadecimal: &amp;gt;&amp;gt;&amp;gt; hex(int(&amp;#39;101010&amp;#39;,2)) &amp;#39;0x2a&amp;#39; 6. Octonary to hexadecimal: &amp;gt;&amp;gt;&amp;gt; hex(int(&amp;#39;17&amp;#39;,8)) &amp;#39;0xf&amp;#39; 7. Decimal to binary: &amp;gt;&amp;gt;&amp;gt; bin(10) &amp;#39;0b1010&amp;#39; 8. Hexadecimal to binary: &amp;gt;&amp;gt;&amp;gt; bin(int(&amp;#39;ff&amp;#39;,16)) &amp;#39;0b11111111&amp;#39; 9. Octonary to binary: &amp;gt;&amp;gt;&amp;gt; bin(int(&amp;#39;17&amp;#39;,8)) &amp;#39;0b1111&amp;#39; 10. Binary to octonary: &amp;gt;&amp;gt;&amp;gt; oct(0b1010) &amp;#39;012&amp;#39; 11.</description>
    </item>
    
    <item>
      <title>Regularization in Deep Learning</title>
      <link>https://billbeatthepeat.github.io/2018/regularization-in-deep-learning/</link>
      <pubDate>Tue, 16 Oct 2018 23:14:02 +0000</pubDate>
      
      <guid>https://billbeatthepeat.github.io/2018/regularization-in-deep-learning/</guid>
      <description>0. Definition of Regularization One of the main goal in machine learning is to not only perform great in tranining set, but also get a good generalization in any test set. According to the book Deep Learning, regularization in deep learning can be described as an modification in learning model aims at reducing generalization error but not training error. Generally, there are two ways of adding regularization in the network. One is to apply extra restrction on the parameters, like $L_1$ and $L_1$, the other is to add an extra iterm in the objective funtions, like $Dropout$.</description>
    </item>
    
    <item>
      <title>Understanding of Convolutional Layer</title>
      <link>https://billbeatthepeat.github.io/2018/understanding-of-convolutional-layer/</link>
      <pubDate>Fri, 05 Oct 2018 10:29:52 +0000</pubDate>
      
      <guid>https://billbeatthepeat.github.io/2018/understanding-of-convolutional-layer/</guid>
      <description>This is a note of the basic knowledge of Convolutional layer in CNN.
Mostly from reference: èƒ½å¦å¯¹å·ç§¯ç¥ç»ç½‘ç»œå·¥ä½œåŸç†åšä¸€ä¸ªç›´è§‚çš„è§£é‡Šï¼Ÿ - Owl of Minervaçš„å›ç­” - çŸ¥ä¹ https://www.zhihu.com/question/39022858/answer/224446917
Partly from notes of book Deep Learning
 1. Nature of the convolutional layer Picture can be stored as a map of values with several channels (1-3, 1 when it is a gray pic, 3 when it is a color picture represent RGB channels). This map of values can be seen as a matrix.</description>
    </item>
    
    <item>
      <title>Heuristic Function</title>
      <link>https://billbeatthepeat.github.io/2018/heuristic-function/</link>
      <pubDate>Sun, 23 Sep 2018 23:01:55 +0000</pubDate>
      
      <guid>https://billbeatthepeat.github.io/2018/heuristic-function/</guid>
      <description>This blog is a simple summary about heuristic function for the lectrure CS118.
1. Definition of Heuristic function According to the Wikipedia, a heuristic method in computer science, especially in artificial intelligence, is a function used to boost the speed of solving a problem or finding an approximate solution when classic methods fails to find any exact solution. &amp;ldquo;This is achieved by trading optimality, completeness, accuracy, or precision for speed. In a way, it can be considered a shortcut.</description>
    </item>
    
    <item>
      <title>Logistic Regression Derived From Linear Regression</title>
      <link>https://billbeatthepeat.github.io/2018/logistic-regression-derived-from-linear-regression/</link>
      <pubDate>Sat, 13 Jan 2018 13:14:50 +0000</pubDate>
      
      <guid>https://billbeatthepeat.github.io/2018/logistic-regression-derived-from-linear-regression/</guid>
      <description>Logistics Regression çš„æ¨å¯¼ï¼ˆfrom Linear Regressionï¼‰ çº¿æ€§å›å½’å’Œé€»è¾‘å›å½’æ˜¯æœºå™¨å­¦ä¹ ç®—æ³•ä¸­å¸¸ç”¨çš„ä¸¤ç§ç®—æ³•ï¼Œåˆ†åˆ«ç”¨äºå›å½’é¢„æµ‹å’Œåˆ†ç±»é—®é¢˜ã€‚å¯¹äºå›å½’é—®é¢˜ï¼Œå½“å›å½’çš„ç›®æ ‡å˜é‡å˜ä¸ºç¦»æ•£ä¸”æœ‰é™çš„å˜é‡æ—¶ï¼Œå›å½’é—®é¢˜å°±å¯ä»¥ç®€åŒ–ä¸ºåˆ†ç±»é—®é¢˜ï¼Œæ•…è€Œåˆ†ç±»é—®é¢˜å¯ä»¥çœ‹ä½œæ˜¯å›å½’é—®é¢˜åœ¨æŸç§æ¡ä»¶ä¸‹çš„ç‰¹ä¾‹ã€‚æ•…è€Œï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ä»ä¸€äº›å›å½’é—®é¢˜çš„æ¨¡å‹ä¸­æ¨å¯¼å‡ºåˆ†ç±»é—®é¢˜çš„æ¨¡å‹ã€‚æœ¬æ–‡ä¸­å°±å°†ä»å›å½’é—®é¢˜ä¸­çš„ç®€å•æ¨¡å‹ â€”â€” çº¿æ€§å›å½’ï¼ˆlinear regressionï¼‰æ¨å¯¼å‡ºç®€å•çš„åˆ†ç±»æ¨¡å‹é€»è¾‘å›å½’ï¼ˆlogistic regressionï¼‰ã€‚
åœ¨ç®€å•çš„äºŒåˆ†ç±»é—®é¢˜ä¸­ï¼Œç›®æ ‡å€¼åˆ†åˆ«ä¸º 1 å’Œ 0 ï¼ˆæˆ– 1 å’Œ -1ï¼‰ã€‚æ•…è€Œå¯ä»¥å°†è¿™ç§äºŒåˆ†ç±»ä»»åŠ¡ç®€å•åœ°ç†è§£æˆä¼°è®¡æŸä¸€äº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡ Pï¼Œé€šè¿‡å¯¹è¿™ä¸€æ¦‚ç‡çš„å¤§å°çš„åŒºåˆ«æ¥è¾¾åˆ°åˆ†ç±»çš„ç›®çš„ã€‚å› æ­¤ï¼Œæ ¹æ®æ¦‚ç‡çš„å®šä¹‰ï¼Œæ¨¡å‹è¾“å‡ºå€¼$y_i$çš„å–å€¼èŒƒå›´å¿…é¡»æ»¡è¶³$$0 &amp;lt; y_i &amp;lt; 1,\ ä¸” \sum_i y_i = 1.$$
ç„¶è€Œï¼Œæ ¹æ®æˆ‘ä»¬å¯¹çº¿æ€§å›å½’æ¨¡å‹çš„äº†è§£:$$P = f_\theta(x) = \theta^T x,$$è¾“å‡ºå€¼çš„èŒƒå›´åº”è¯¥æ˜¯åœ¨$[-inf, +inf]$ä¹‹é—´ï¼Œå¾ˆæ˜æ˜¾ä¸æ»¡è¶³äºåˆ†ç±»ï¼Œå°¤å…¶æ˜¯äºŒåˆ†ç±»é—®é¢˜çš„è¦æ±‚ã€‚ æ‰€ä»¥ä¸ºäº†ä½¿å¾—çº¿æ€§æ¨¡å‹åœ¨åˆ†ç±»é—®é¢˜ä¸­å¯ç”¨ï¼Œæˆ‘ä»¬åº”è¯¥å¯¹å…¶è¾“å‡ºå€¼åšå‡ºé™å®šï¼Œå³ï¼š$$logistic(x) = g(f_\theta (x)) = g(\theta^T x)$$ã€‚æ¥ä¸‹æ¥æ˜¯å¯¹$g(x)$çš„æ¨å¯¼ã€‚
æˆ‘ä»¬å¯ä»¥ä»åé¢è€ƒè™‘ï¼Œé€šè¿‡å¯¹åˆ†ç±»æ¨¡å‹çš„å› å˜é‡å€¼åŸŸé™åˆ¶çš„ç§»é™¤è¿›è¡Œå˜æ¢æ¥åæ¨å‡ºæˆ‘ä»¬æ‰€éœ€è¦çš„$g(x)$ã€‚
é¦–å…ˆï¼Œé€‰ç”¨ä¼˜åŠ¿æ¯” Odds ä»£æ›¿æ¦‚ç‡ã€‚ä¼˜åŠ¿æ¯”å°±æ˜¯æ—¶é—´å‘ç”Ÿæ¦‚ç‡å’Œä¸å‘ç”Ÿæ¦‚ç‡ä¹‹é—´çš„æ¯”å€¼ï¼Œè®°ä½œï¼š$$odds = \frac{P}{1-P}$$ã€‚é€šè¿‡æ”¹å˜æ¢ï¼Œæˆ‘ä»¬å¯ä»¥å°†[0,1]ä¹‹é—´çš„ä»»æ„æ•°æ˜ å°„åˆ°[0, +inf]ä¹‹é—´çš„ä»»æ„å®æ•°ã€‚ä½†æ˜¯çº¿æ€§å›å½’çš„è¾“å‡ºè¿˜å¯ä»¥æ˜¯è´Ÿæ•°ï¼Œæ•…è€Œè¿˜éœ€è¦å¦ä¸€æ­¥å˜æ¢å°†[0,inf]çš„å®æ•°åŸŸæ˜ å°„åˆ°æ•´ä¸ªå®æ•°åŸŸç©ºé—´ã€‚
åœ¨ä¼—å¤šéçº¿æ€§å‡½æ•°ä¸­ï¼Œlogå‡½æ•°çš„å€¼åŸŸä¸ºæ•´ä¸ªå®æ•°åŸŸä¸”å•è°ƒï¼Œå› æ­¤å¯ä»¥é€šè¿‡è®¡ç®—ä¼˜åŠ¿æ¯”çš„å¯¹æ•°ï¼Œå¾—åˆ°å‰é¢æ‰€éœ€è¦çš„å˜æ¢ï¼š $$\eta = log(odds) = log\frac{P}{1-P} = logit(P),$$ å…¶ä¸­logitå‡½æ•°è¡¨ç¤ºåˆ†å¯¹æ•°ã€‚ç»è¿‡ä»¥ä¸Šä¸¤æ­¥ï¼Œæˆ‘ä»¬å·²ç»å°†å› å˜é‡çš„å€¼åŸŸé™åˆ¶å»æ‰ï¼Œå³å°†å› å˜é‡çš„å€¼åŸŸ[0ï¼Œ1]ï¼Œæ˜ å°„åˆ°äº†ä¸€ä¸ª[-inf, +inf]ä¸Šã€‚å¦‚æœæ¦‚ç‡ç­‰äº0ï¼Œé‚£ä¹ˆä¼˜åŠ¿æ¯”ä¸º0ï¼Œlogitçš„å€¼ä¸º-infï¼›å¦‚æœæ¦‚ç‡ç­‰äº1ï¼Œé‚£ä¹ˆä¼˜åŠ¿æ¯”ä¸º+infï¼Œlogitçš„å€¼ä¹Ÿä¸º+infï¼Œå½“æ¦‚ç‡å€¼å°äº0.5æ—¶ï¼Œlogitçš„å€¼ä¸ºè´Ÿæ•°ï¼Œåä¹‹ï¼Œä¸ºæ­£æ•°ã€‚
æ¥ç€ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡åæ¨ï¼Œä»çº¿æ€§å›å½’çš„æ¨¡å‹å‡½æ•°ä¸­å¾—åˆ°é€»è¾‘å›å½’çš„æ¨¡å‹çš„æ•°å­¦è¡¨è¾¾ï¼š $$logit(p) = log(\frac{p}{1-p}) = \eta = f_\theta(x) = \theta^T x,$$ $$\Rightarrow p = g(x) = antilogit(x)$$ $$\Rightarrow \frac{p}{1-p} = e^\eta$$ $$\Rightarrow p = \frac{e^\eta}{1 + e^\eta} = \frac{e^{\theta^T \cdot x}}{1 + e^{\theta^T \cdot x}} = sigmoid(x)$$</description>
    </item>
    
    <item>
      <title>Ensemble Methods</title>
      <link>https://billbeatthepeat.github.io/2017/ensemble-methods/</link>
      <pubDate>Thu, 12 Oct 2017 11:25:04 +0000</pubDate>
      
      <guid>https://billbeatthepeat.github.io/2017/ensemble-methods/</guid>
      <description>This blog is a simple summary of the ensemble methods.
1. Basic methods for ensemble 1.1 Voting Voting is to get the ensemble result of a classification problem by gethering all the results from each classifier. And then make the label with most votes the final result.
1.2 Averaging Averaging is to take the mathematical average of all the output of regressors to be the final result of a regression problem.</description>
    </item>
    
    <item>
      <title>Attention in Deep Learning</title>
      <link>https://billbeatthepeat.github.io/2017/attention-in-deep-learning/</link>
      <pubDate>Mon, 25 Sep 2017 18:16:28 +0000</pubDate>
      
      <guid>https://billbeatthepeat.github.io/2017/attention-in-deep-learning/</guid>
      <description>This article is a digest/note for the blog of Denny Britz: Attention ad Memory in Deep Learning and NLP.
 In Neural Machine Translation system, we map the meaning of a sentence into a fixed-length vector representation and then generate a translation base on it.
The first word of English translation is probably highly correlated ith the first word of the source sentence. Researches have found that reversing the source sequence (feeding it backwards into the encoder) produce significantly better results because it shorten the path from the decoder to the relevan parts of encoder.</description>
    </item>
    
    <item>
      <title>Little Skills</title>
      <link>https://billbeatthepeat.github.io/2017/little-skills/</link>
      <pubDate>Mon, 25 Sep 2017 12:07:14 +0000</pubDate>
      
      <guid>https://billbeatthepeat.github.io/2017/little-skills/</guid>
      <description>This is a blog of summary of the little skills which are useful in daily life.
1. Connect the keyboard to the Ubuntu system computer with bluetooth 1. sudo apt-get install bluez-hcidump 2. sudo hcidump -at Then connect the keyboard and monitor the output.  Referenceï¼šUbuntu ä¸‹è¿æ¥è“ç‰™é”®ç›˜</description>
    </item>
    
    <item>
      <title>Activations in Neural Network</title>
      <link>https://billbeatthepeat.github.io/2017/activations-in-neural-network/</link>
      <pubDate>Fri, 15 Sep 2017 11:31:38 +0000</pubDate>
      
      <guid>https://billbeatthepeat.github.io/2017/activations-in-neural-network/</guid>
      <description>Instruction All of us have encountered with the problem of choosing an activation function during the building of neural network models. But what is activation functions, why we should use them and which should we choose in the network? Here is some of the summary or notes wrote after reading the blog and the discussion.
0. Structure  What? Why? Which? Reference  1. What is Activation Function  In biologically inspired neural networks, the activation function is usually an abstraction representing the rate of action potential firing in the cell.</description>
    </item>
    
    <item>
      <title>Trade-off of Batch Size</title>
      <link>https://billbeatthepeat.github.io/2017/trade-off-of-batch-size/</link>
      <pubDate>Fri, 08 Sep 2017 16:26:05 +0000</pubDate>
      
      <guid>https://billbeatthepeat.github.io/2017/trade-off-of-batch-size/</guid>
      <description>According to the answer on Quora, the affection of batch size in the training of ANN are mainly two points:
 Reduce the variance of the stochastic gradient updates. By taking average of some functions over each training example in the batch, using mini-batches can efficiently reduce the noise of finding the best direction to take. As we know, SGD needs to find the way out to make steepest descent in gradient.</description>
    </item>
    
    <item>
      <title>äº•ä¸Šç›´ä¹…</title>
      <link>https://billbeatthepeat.github.io/2017/%E4%BA%95%E4%B8%8A%E7%9B%B4%E4%B9%85/</link>
      <pubDate>Tue, 29 Aug 2017 16:35:50 +0000</pubDate>
      
      <guid>https://billbeatthepeat.github.io/2017/%E4%BA%95%E4%B8%8A%E7%9B%B4%E4%B9%85/</guid>
      <description>Naohisa Inoue (äº•ä¸Š ç›´ä¹… Inoue Naohisa, born in 1948 in Osaka, Japan) is a fantasy artist influenced by both the Surrealism and Impressionism movements.
2017.8.26, meet in M50 CREATIVE PARK.
Iblard Jikan (ã‚¤ãƒãƒ©ãƒ¼ãƒ‰æ™‚é–“ IbarÄdo Jikan, literally &amp;ldquo;Iblard Time&amp;rdquo;) is a Japanese anime OVA produced by Studio Ghibli, released in Japan on DVD and Blu-ray disc on July 4, 2007, as part of the &amp;ldquo;Ghibli ga Ippai Collection&amp;rdquo;.[1] It is directed by Naohisa Inoue.</description>
    </item>
    
  </channel>
</rss>